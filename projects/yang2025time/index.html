<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
<style>
  .publication-links {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 12px;
      margin-top: 10px;
  }
  .link-block .button {
      display: inline-flex;
      align-items: center;
      gap: 8px;
  }
</style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding</h1>
          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mwxely.github.io/" target="_blank">Zuhao Yang</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://yingchen001.github.io/" target="_blank">Yingchen Yu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yunqing-me.github.io/" target="_blank">Yunqing Zhao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://personal.ntu.edu.sg/shijian.lu/" target="_blank">Shijian Lu</a><sup>1✝</sup>,
            </span>
            <span class="author-block">
              <a href="https://songbai.site/" target="_blank">Song Bai</a><sup>2</sup>
            </span>
          </div>
          <!-- Affiliations -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Nanyang Technological University, Singapore</span><br>
            <span class="author-block"><sup>2</sup>ByteDance, Singapore</span>
            <span class="eql-cntrb">
              <small><br><sup>*</sup>This work was done while Zuhao Yang was interning at ByteDance.</small>
            </span>
            <span class="eql-cntrb">
              <small><br><sup>✝</sup>Shijian Lu is the corresponding author.</small>
            </span>
            <br>
            <span class="author-block"><strong>ICCV 2025</strong></span>
          </div>
          <!-- Links -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2508.01699"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2508.01699"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Poster -->
              <span class="link-block">
                <a href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/1752.png?t=1759818915.441532"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-image"></i></span>
                  <span>Poster</span>
                </a>
              </span>
              <!-- Slides -->
              <span class="link-block">
                <a href="https://iccv.thecvf.com/media/iccv-2025/Slides/1752.pdf"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-powerpoint"></i></span>
                  <span>Slides</span>
                </a>
              </span>
              <!-- Video -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=YODyaExFKSU"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-video"></i></span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video Temporal Grounding (VTG) aims to precisely identify video event segments in response to textual queries. The outputs of VTG tasks manifest as sequences of events, each defined by precise timestamps, saliency scores, and textual descriptions. Despite recent advances, a fundamental limitation persists in existing Video Large Language Models (Video-LLMs): they process all task tokens through identical and static pathways, failing to recognize that temporal localization, saliency assessment, and textual generation represent fundamentally distinct tasks requiring specialized processing. To address this, we introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that effectively decomposes VTG tasks by dynamically routing task-specific tokens (e.g., timestamps, saliency scores) to specialized experts, with increased computational efficiency. Our design choices enable precise handling of each subtask, leading to improved event modeling across diverse VTG applications. Extensive experiments demonstrate that TimeExpert consistently achieves state-of-the-art performance on various VTG tasks such as Dense Video Captioning, Moment Retrieval, and Video Highlight Detection.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
        <h2 class="title is-3">Video Temporal Grounding (VTG)</h2>
        <center>
        <img src="static/images/teaser.png" alt="Task Definition of TimeExpert." class="center-image blend-img-background"/>
        </center>
        <div class="level-set has-text-justified">
          <p>
            <strong>Left:</strong> Video Temporal Grounding (VTG) is a fine‑grained video understanding task that aims to accurately locate content along with event timestamps based on natural language queries. In this work, we mainly consider three major types of VTG tasks: (1) Moment Retrieval (MR), (2) Video Highlight Detection (VHD), and (3) Dense Video Captioning (DVC). The outputs of VTG often contain textual captions, <span style="color:FireBrick">timestamps</span>, and <span style="color:blue">saliency scores</span>. <strong>Right:</strong> Unlike existing methods (e.g., TimeChat) that employ a single <em>static</em> model, motivated by expert specialization on different task tokens, we propose <em>TimeExpert</em>, an expert‑guided Video LLM with <em>dynamic</em> token routing. Through task‑aware expert allocation, TimeExpert demonstrates substantial improvements over state‑of‑the‑art Video‑LLMs on several VTG benchmarks. For example, here we visualize zero‑shot F1 score for DVC on the YouCook2 dataset, R@1<sub>IoU=0.7</sub> for MR on the Charades‑STA dataset, and HIT@1 for VHD on the QVHighlights dataset. More results and analysis are in the experimental section.
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
        <h2 class="title is-3">Comparison across VTG Approaches</h2>
        <center>
        <img src="static/images/method.png" alt="Comparison of VTG Approaches." class="center-image blend-img-background"/>
        </center>
        <div class="level-set has-text-justified">
          <p>
            <strong>(a):</strong> VTG‑specific Video‑LLM relies on a single static model with shared parameters for all tasks, limiting its ability to specialize across diverse VTG subtasks. <strong>(b):</strong> Vanilla MoE improves upon this by activating a fixed set (e.g., k=2) of experts, enabling a certain degree of task specialization. <strong>(c):</strong> Our <em>TimeExpert</em> goes further, implementing adaptive routing that dynamically allocates new experts when tokens lack suitable matches and prunes unmatched experts when necessary. This dynamic design significantly enhances computational efficiency while achieving superior specialization—especially for VTG subtasks that require distinct feature representations.
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
        <h2 class="title is-3">Architecture Overview of TimeExpert</h2>
        <center>
        <img src="static/images/framework.png" alt="Framework Overview of TimeExpert." class="center-image blend-img-background"/>
        </center>
        <div class="level-set has-text-justified">
          <p>
            Our model leverages independent encoders and decoding heads to process time, score, and text inputs and outputs. The timestamps and saliency scores of sampled frames are encoded into special tokens and integrated into the corresponding visual tokens. During inference, the generated response follows a structured format, sequentially incorporating time tokens, score tokens, and text tokens.
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{yang2025timeexpert,
        title={TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding},
        author={Yang, Zuhao and Yu, Yingchen and Zhao, Yunqing and Lu, Shijian and Bai, Song},
        booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
        year={2025}
      }
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
