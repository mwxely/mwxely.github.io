---
permalink: /
title: ""
excerpt: "I am a second-year Ph.D. Student at NTU. My research interests include: video understanding, video reasoning, and video generation."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
.paper-box {
  display: flex;
  align-items: flex-start;
  margin-bottom: 24px;
  border-radius: 12px;
  box-shadow: 0 4px 16px 0 rgba(0,0,0,0.08);
  background: #fff;
  padding: 16px;
  gap: 20px;
}
.paper-box-image {
  flex: 0 0 220px;
  position: relative;
}
.paper-box-image .badge {
  position: absolute;
  left: 0; top: 0;
  background: #337ab7;
  color: #fff;
  padding: 2px 12px;
  font-size: 14px;
  border-radius: 8px 0 8px 0;
  font-weight: 700;
  z-index: 2;
}
.paper-box-image img {
  width: 100%;
  border-radius: 8px;
  margin-top: 22px;
}
.paper-box-text {
  flex: 1;
  font-size: 0.98em;
  line-height: 1.45;
  color: #202124;
}
.paper-box-text a {
  color: #1684fc;
  font-weight: 600;
  font-size: 1em;
}
.paper-box-text i {
  color: #555;
  font-style: italic;
  font-size: 0.98em;
}
</style>

I am a second-year Ph.D. student with [Visual Intelligence Lab](https://sg-vilab.github.io/) at Nanyang Technological University (NTU), supervised by [Prof. Shijian Lu](https://personal.ntu.edu.sg/shijian.lu/). Prior to joining NTU, I obtained my B.S. degree in Computing Science from University of Alberta. I also work closely with [Dr. Lidong Bing](https://lidongbing.github.io/) at [MiroMind.ai](https://miromind.ai/) and [Dr. Song Bai](https://songbai.site/) when he was at ByteDance. My research centers on the long-standing quest for building video-centric multimodal intelligence, spanning controllable generation, temporal reasoning, agentic tool use, and long-term memory.

I enjoy collaborating with self-motivated researchers at [LMMs-Lab](https://www.lmms-lab.com/), a non-profit open-source organization led by [Bo Li](https://brianboli.com/) and [Prof. Ziwei Liu](https://liuziwei7.github.io/). Our mission is to advance large multimodal models with a shared vision of *Feeling the AGI*. We are actively looking for like-minded individuals to contribute to the community together!

ğŸ”¥ Exciting News
---
* 2025.10 - Four papers were released, focusing on multimodal reasoning ([OpenMMReasoner](https://evolvinglmms-lab.github.io/OpenMMReasoner/)), multimodal agentic tool use ([LongVT](https://evolvinglmms-lab.github.io/LongVT/)), and visual token redundancy in both MLLMs ([ToDRE](https://arxiv.org/abs/2505.18757)) and [diffusion-based MLLMs](https://arxiv.org/abs/2511.15098).
* 2025.10 - One paper was accepted by **SIGGRAPH Asia 2025**.
* 2025.08 - One paper was accepted by **EMNLP 2025**.
* 2025.06 - Two papers were accepted by **ICCV 2025**.
* 2025.05 - Two papers were accepted by **ACL 2025**.
* 2023.09 - One paper was accepted by **NeurIPS 2023**.


ğŸ“ Selected Publications ([Full List](https://mwxely.github.io/publications/))
---
<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">Preprint</span>
    <img src="images/LVT.png" alt="LVT" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://arxiv.org/abs/2511.20785"><b>LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling</b></a><br>
    <b>Zuhao Yang*</b>, Sudong Wang*, Kaichen Zhang*, Keming Wu, Sicong Leng, Yifan Zhang, Chengwei Qin, Shijian Lu, Xingxuan Li, Lidong Bing<br>
    PreprintÂ 2025<br>
    <a href="https://arxiv.org/pdf/2511.20785">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/yang2025longvt.html">bibtex</a>Â /Â <a href="https://github.com/EvolvingLMMs-Lab/LongVT">code</a>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">Preprint</span>
    <img src="images/OMR.png" alt="OMR" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://arxiv.org/abs/2511.16334"><b>OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe</b></a><br>
    Kaichen Zhang*, Keming Wu*, <b>Zuhao Yang</b>, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, Lidong Bing<br>
    PreprintÂ 2025<br>
    <a href="https://arxiv.org/pdf/2511.16334">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/zhang2025openmmreasoner.html">bibtex</a>Â /Â <a href="https://github.com/EvolvingLMMs-Lab/OpenMMReasoner">code</a>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">Preprint</span>
    <img src="images/dMLLM.png" alt="dMLLM" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://arxiv.org/abs/2511.15098"><b>A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models</b></a><br>
    Duo Li*, <b>Zuhao Yang*</b>, Xiaoqin Zhang, Ling Shao, Shijian Lu<br>
    PreprintÂ 2025<br>
    <a href="https://arxiv.org/pdf/2511.15098">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/li2025dmllm.html">bibtex</a>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">Preprint</span>
    <img src="images/ToDRE.png" alt="ToDRE" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://arxiv.org/abs/2505.18757"><b>ToDRE: Effective Visual Token Pruning via Token Diversity and Task Relevance</b></a><br>
    Duo Li*, <b>Zuhao Yang*</b>, Xiaoqin Zhang, Ling Shao, Shijian Lu<br>
    PreprintÂ 2025<br>
    <a href="https://arxiv.org/pdf/2505.18757">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/li2025todre.html">bibtex</a>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">ICCV</span>
    <img src="images/TE.png" alt="TimeExpert" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://arxiv.org/abs/2508.01699"><b>TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding</b></a><br>
    <b>Zuhao Yang</b>, Yingchen Yu, Yunqing Zhao, Shijian Lu, Song Bai<br>
    ICCVÂ 2025<br>
    <a href="https://arxiv.org/pdf/2508.01699">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/yang2025timeexpert.html">bibtex</a>Â /Â <a href="https://mwxely.github.io/projects/yang2025time/index">webpage</a>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">ICCV</span>
    <img src="images/VTG.png" alt="VTG" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://arxiv.org/abs/2508.01698"><b>Versatile Transition Generation with Image-to-Video Diffusion</b></a><br>
    <b>Zuhao Yang</b>, Jiahui Zhang, Yingchen Yu, Shijian Lu, Song Bai<br>
    ICCVÂ 2025<br>
    <a href="https://arxiv.org/pdf/2508.01698">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/yang2025versatile.html">bibtex</a>Â /Â <a href="https://mwxely.github.io/projects/yang2025vtg/index">webpage</a>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">ACL</span>
    <img src="images/QAEval.png" alt="QAEval" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://aclanthology.org/2025.acl-long.716"><b>QAEval: Mixture of Evaluators for Questionâ€‘Answering Task Evaluation</b></a><br>
    Tan Yue, Rui Mao, Xuzhao Shi, Shuo Zhan, <b>Zuhao Yang</b>, Dongyan Zhao<br>
    ACLÂ 2025<br>
    <a href="https://aclanthology.org/2025.acl-long.716.pdf">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/yue2025qaeval.html">bibtex</a> /Â <a href="https://github.com/yuetanbupt/QAEval">code</a>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <span class="badge">NeurIPS</span>
    <img src="images/FACE.png" alt="FACE" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://arxiv.org/abs/2305.10307"><b>FACE: Evaluating Natural Language Generation with Fourier Analysis of Crossâ€‘Entropy</b></a><br>
    <b>Zuhao Yang*</b>, Yingfang Yuan*, Yang Xu*, Shuo Zhan, Huajun Bai, Kefan Chen<br>
    NeurIPSÂ 2023<br>
    <a href="https://arxiv.org/pdf/2305.10307">paper</a>Â /Â <a href="https://mwxely.github.io/bibtex/yang2023face.html">bibtex</a>Â /Â <a href="https://github.com/CLCS-SUSTech/FACE">code</a>
  </div>
</div>

ğŸ“– Educational Background
---
* **2024.01 - Present:** Doctor of Philosophy, College of Computing and Data Science, Nanyang Technological University
* **2022.08 - 2024.01:** Master in Artificial Intelligence, College of Computing and Data Science, Nanyang Technological University
* **2017.09 - 2021.06:** Bachelor in Computing Science, Department of Computing Science, University of Alberta

ğŸ§‘â€âš–ï¸ Working Experiences
---
* **2025.04 - Present:** AI Scientist Intern, Shanda AI Research Institute & MiroMind.ai, Singapore
* **2023.11 - 2025.03:** AI Research Intern, ByteDance Inc. & TikTok, Singapore
* **2021.05 - 2022.06:** NLP Algorithm Engineer, TMI Robotics Technology, Shanghai

ğŸ’» Academic Services
---
**Conference Reviewer**
* CVPR 24/25/26, ECCV 24, ACMMM 24, NeurIPS 24/25, ICLR 25, AISTATS 25/26, ICML 25, ICCV 25  

**Journal Reviewer**
* IEEE TPAMI, Pattern Recognition, Journal of Electronic Imaging  

**Workshop PC Member**
* [SyntaGen: Harnessing Generative Models for Synthetic Visual Datasets](https://syntagen25.github.io/) (CVPR 24/25)
* [Neural Rendering Intelligence](https://neural-rendering.com/) (CVPR 24)

**Teaching Assistant**
* AI6121 - Computer Vision, NTU, 2025 Fall

ğŸ† Patent & Awards
---
* Method, Device, and Medium for Video Temporal Grounding with Mixture-of-Experts, US Patent, 2025
* Method, Device, and Medium for Generating Transition Videos with Diffusion Model, SG Patent, 2024
* [Method, Device, and Medium for Automatic Question-Answering](http://epub.cnipa.gov.cn/patent/CN113946669A), CN Patent, 2022
* Outstanding Graduate, University of Alberta, 2021
* Dean's Honor Roll Award, University of Alberta, 2018 - 2020
* International Student Scholarship, University of Alberta, 2017 - 2019
